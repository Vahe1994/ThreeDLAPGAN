{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import sys\n",
    "sys.path.append(\"/home/service/\")\n",
    "from ThreeDLAPGAN.external.LGan.src.ae_templates import mlp_architecture_ala_iclr_18, default_train_params\n",
    "from ThreeDLAPGAN.external.LGan.src.autoencoder import Configuration as Conf\n",
    "from ThreeDLAPGAN.external.LGan.src.point_net_ae import PointNetAutoEncoder\n",
    "\n",
    "from  ThreeDLAPGAN.external.LGan.src.in_out import snc_category_to_synth_id, create_dir, PointCloudDataSet, \\\n",
    "                                        load_all_point_clouds_under_folder\n",
    "from  ThreeDLAPGAN.external.LGan.src.in_out import  PointCloudDataSet\n",
    "\n",
    "\n",
    "from ThreeDLAPGAN.external.LGan.src.tf_utils import reset_tf_graph\n",
    "import tqdm\n",
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_out_dir = '../data/'                        # Use to write Neural-Net check-points etc.\n",
    "top_in_dir = '../data/shape_net_core_uniform_samples_2048/' # Top-dir of where point-clouds are stored.\n",
    "\n",
    "experiment_name = 'single_class_ae'\n",
    "n_pc_points = 2048                              # Number of points per model.\n",
    "bneck_size = 128                                # Bottleneck-AE size\n",
    "ae_loss = 'emd'                             # Loss to optimize: 'emd' or 'chamfer'\n",
    "class_name = raw_input('Give me the class name (e.g. \"chair\"): ').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_id = snc_category_to_synth_id()[class_name]\n",
    "class_dir = osp.join(top_in_dir , syn_id)\n",
    "print(class_dir)\n",
    "all_pc_data = load_all_point_clouds_under_folder(class_dir, n_threads=8, file_ending='.ply', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pc_data.point_clouds[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make downsampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fool Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_count = int(raw_input('Give me the count of points you want to be in point_cloud e.g. 1024: ').lower())\n",
    "assert pc_count <= all_pc_data.n_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclouds = np.empty([all_pc_data.point_clouds.shape[0],pc_count, all_pc_data.point_clouds.shape[2]], dtype=np.float32)\n",
    "for i,pc in enumerate(all_pc_data.point_clouds):\n",
    "    mask = np.zeros(p\n",
    "                    c.shape[0])\n",
    "    a = np.random.choice(pc.shape[0],size = pc_count,replace=False)\n",
    "    mask[a] = 1\n",
    "    pclouds[i] = pc[mask.astype(bool)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart downsampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_pc_data.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pclouds_smart = np.empty([all_pc_data.point_clouds.shape[0],pc_count, all_pc_data.point_clouds.shape[2]], dtype=np.float32)\n",
    "from sklearn.cluster import KMeans\n",
    "from tqdm import tqdm\n",
    "def get_smart_downsample(data,pc_count):\n",
    "    buf_size=1\n",
    "    fout = open('log.txt', 'wb', buf_size)\n",
    "    pclouds_smart = np.empty([data.shape[0],pc_count, data.shape[2]], dtype=np.float32)\n",
    "    for i,pc in tqdm(enumerate(data)):\n",
    "        while True:\n",
    "            try:\n",
    "                fout.write(str(i)+'\\n')\n",
    "                kmeans  = KMeans(n_clusters = pc_count,n_jobs=-1)\n",
    "                kmeans.fit(pc)\n",
    "                pclouds_smart[i] = kmeans.cluster_centers_ \n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "    fout.close()\n",
    "    return pclouds_smart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pclouds_smart512 = get_smart_downsample(pclouds_smart1024,512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load pickled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint, pickle\n",
    "def read_pickle_data(input='data.pkl'):\n",
    "    pkl_file = open(input, 'rb')\n",
    "\n",
    "    pclouds = pickle.load(pkl_file)\n",
    "\n",
    "    pkl_file.close()\n",
    "    return pclouds\n",
    "\n",
    "def pickle_data(pcloud_smart=None,output='data.pkl'):\n",
    "    output = open(output, 'wb')\n",
    "    pickle.dump(pcloud_smart, output)\n",
    "    output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_data(all_pc_data.point_clouds,'pclouds_smart2048.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pclouds_smart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclouds_smart2048 = read_pickle_data(input='pclouds_smart2048.pkl')\n",
    "pclouds_smart1024 = read_pickle_data(input='pclouds_smart1024.pkl')\n",
    "pclouds_smart512 = read_pickle_data(input='pclouds_smart512.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pclouds_smart2048 = all_pc_data.point_clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import numpy as np\n",
    "import plotly.offline as offline\n",
    "\n",
    "x, y, z = pclouds_smart2048[0,:,0],pclouds_smart2048[0,:,1],pclouds_smart2048[0,:,2]\n",
    "trace1 = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        line=dict(\n",
    "            color='rgba(217, 217, 217, 0.14)',\n",
    "            width=0.5\n",
    "        ),\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='smart_downsamplingv_2048')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import numpy as np\n",
    "import plotly.offline as offline\n",
    "\n",
    "x, y, z = pclouds_smart[0,:,0],pclouds_smart[0,:,1],pclouds_smart[0,:,2]\n",
    "trace1 = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        line=dict(\n",
    "            color='rgba(217, 217, 217, 0.14)',\n",
    "            width=0.5\n",
    "        ),\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='smart_downsamplingv_512')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plotly.tools.set_credentials_file(username='Vahe1994', api_key='MTEFahTX43GTctGnrfox')\n",
    "\n",
    "x, y, z = pclouds[0,:,0],pclouds[0,:,1],pclouds[0,:,2]\n",
    "trace1 = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        line=dict(\n",
    "            color='rgba(217, 217, 217, 0.14)',\n",
    "            width=0.5\n",
    "        ),\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='simple-3d-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking Resul from downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = pclouds[0,:,0],pclouds[0,:,1],pclouds[0,:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "plotly.tools.set_credentials_file(username='Vahe1994', api_key='MTEFahTX43GTctGnrfox')\n",
    "\n",
    "x, y, z = pclouds[0,:,0],pclouds[0,:,1],pclouds[0,:,2]\n",
    "trace1 = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        line=dict(\n",
    "            color='rgba(217, 217, 217, 0.14)',\n",
    "            width=0.5\n",
    "        ),\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='simple-3d-scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minim_pc_data = PointCloudDataSet(pclouds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pc_points = 1024\n",
    "train_dir = create_dir(osp.join(top_out_dir, experiment_name))\n",
    "train_params = default_train_params()\n",
    "encoder, decoder, enc_args, dec_args = mlp_architecture_ala_iclr_18(n_pc_points, bneck_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = Conf(n_input = [n_pc_points, 3],\n",
    "            loss = ae_loss,\n",
    "            training_epochs = train_params['training_epochs'],\n",
    "            batch_size = train_params['batch_size'],\n",
    "            denoising = train_params['denoising'],\n",
    "            learning_rate = train_params['learning_rate'],\n",
    "            train_dir = train_dir,\n",
    "            loss_display_step = train_params['loss_display_step'],\n",
    "            saver_step = train_params['saver_step'],\n",
    "            z_rotate = train_params['z_rotate'],\n",
    "            encoder = encoder,\n",
    "            decoder = decoder,\n",
    "            encoder_args = enc_args,\n",
    "            decoder_args = dec_args\n",
    "           )\n",
    "conf.experiment_name = experiment_name\n",
    "conf.held_out_step = 5              # How often to evaluate/print out loss on held_out data (if any).\n",
    "conf.save(osp.join(train_dir, 'configuration'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf_graph()\n",
    "ae = PointNetAutoEncoder(conf.experiment_name, conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf_size = 1 # flush each line\n",
    "fout = open(osp.join(conf.train_dir, 'train_stats.txt'), 'a', buf_size)\n",
    "train_stats = ae.train(minim_pc_data, conf, log_file=fout)\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minim_pc_data.point_clouds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_pc, feed_model_names, _ = minim_pc_data.next_batch(10)\n",
    "reconstructions = ae.reconstruct(feed_pc)\n",
    "latent_codes = ae.transform(feed_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_codes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = ae.decode(latent_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import numpy as np\n",
    "plotly.tools.set_credentials_file(username='Vahe1994', api_key='MTEFahTX43GTctGnrfox')\n",
    "\n",
    "x, y, z = points[0,:,0],points[0,:,1],points[0,:,2]\n",
    "trace1 = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        line=dict(\n",
    "            color='rgba(217, 217, 217, 0.14)',\n",
    "            width=0.5\n",
    "        ),\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='simple-3d-scatter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look how work decoder on when input is random distrbution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = ae.decode(np.abs(np.random.normal(0, 1, (10, 128))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y, z = points[0,:,0],points[0,:,1],points[0,:,2]\n",
    "trace1 = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        line=dict(\n",
    "            color='rgba(217, 217, 217, 0.14)',\n",
    "            width=0.5\n",
    "        ),\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='simple-3d-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see nothing meaningfull "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restore AE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf_graph()\n",
    "ae = PointNetAutoEncoder(conf.experiment_name, conf)\n",
    "ae.restore_model(model_path='/home/service/ThreeDLAPGAN/data/single_class_ae/',epoch=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import numpy as np\n",
    "plotly.tools.set_credentials_file(username='Vahe1994', api_key='MTEFahTX43GTctGnrfox')\n",
    "\n",
    "x, y, z = points[0,:,0],points[0,:,1],points[0,:,2]\n",
    "trace1 = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        line=dict(\n",
    "            color='rgba(217, 217, 217, 0.14)',\n",
    "            width=0.5\n",
    "        ),\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='drom restored-3d-scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment with latent pace,that show smoothness of space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took two random smaples from training sets get their vector's in latent space. After that we  construct mean of this two vectors and decode it. As you can see  it's very much alike of chair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points= []\n",
    "\n",
    "latent_codes[2] = (latent_codes[0]+latent_codes[1])/2\n",
    "points = ae.decode(latent_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x, y, z = points[0,:,0],points[0,:,1],points[0,:,2]\n",
    "trace1 = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        line=dict(\n",
    "            color='rgba(217, 217, 217, 0.14)',\n",
    "            width=0.5\n",
    "        ),\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = points[1,:,0],points[1,:,1],points[1,:,2]\n",
    "trace1 = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        line=dict(\n",
    "            color='rgba(217, 217, 217, 0.14)',\n",
    "            width=0.5\n",
    "        ),\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = points[2,:,0],points[2,:,1],points[2,:,2]\n",
    "trace1 = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        line=dict(\n",
    "            color='rgba(217, 217, 217, 0.14)',\n",
    "            width=0.5\n",
    "        ),\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='merged')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ThreeDLAPGAN.src.generator_discriminator import discriminator_smpl,generator_smpl\n",
    "from ThreeDLAPGAN.external.LGan.src.latent_gan import LatentGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_pc, feed_model_names, _ = minim_pc_data.next_batch(1200)\n",
    "reconstructions = ae.reconstruct(feed_pc)\n",
    "latent_codes = ae.transform(feed_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'GAN' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = create_dir(osp.join(top_out_dir, experiment_name))\n",
    "train_params = default_train_params()\n",
    "# generator, discriminator, generator_args, discriminator_args = Simple_Gan(128)\n",
    "generator = generator_smpl\n",
    "discriminator = discriminator_smpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_tf_graph()\n",
    "gan = LatentGAN(name='latent_gan', learning_rate=0.0001, n_output=[128], noise_dim=128, discriminator=discriminator, generator=generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = PointCloudDataSet(latent_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_params = {  'mu':0, 'sigma':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buf_size = 1 # flush each line\n",
    "train_stats=[]\n",
    "fout = open(osp.join(conf.train_dir, 'train_stats.txt'), 'a', buf_size)\n",
    "for i in range(1000):\n",
    "    feed_pc, feed_model_names, _ = minim_pc_data.next_batch(1000)\n",
    "    reconstructions = ae.reconstruct(feed_pc)\n",
    "    latent_codes = ae.transform(feed_pc)\n",
    "    lc = PointCloudDataSet(latent_codes)\n",
    "    train_stats=gan._single_epoch_train(lc,  batch_size=500, noise_params=noise_params)\n",
    "    if(i%5==0):\n",
    "        print(train_stats)\n",
    "        train_stats=[]\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_params = {  'mu':0, 'sigma':1}\n",
    "p = gan.generate(1,noise_params=noise_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points =ae.decode(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y, z = points[0,:,0],points[0,:,1],points[0,:,2]\n",
    "trace1 = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        line=dict(\n",
    "            color='rgba(217, 217, 217, 0.14)',\n",
    "            width=0.5\n",
    "        ),\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='generated_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = points[0,:,0],points[0,:,1],points[0,:,2]\n",
    "trace1 = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        line=dict(\n",
    "            color='rgba(217, 217, 217, 0.14)',\n",
    "            width=0.5\n",
    "        ),\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='generated_v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = points[0,:,0],points[0,:,1],points[0,:,2]\n",
    "trace1 = go.Scatter3d(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    z=z,\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=4,\n",
    "        line=dict(\n",
    "            color='rgba(217, 217, 217, 0.14)',\n",
    "            width=0.5\n",
    "        ),\n",
    "        opacity=0.8\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "data = [trace1]\n",
    "layout = go.Layout(\n",
    "    margin=dict(\n",
    "        l=0,\n",
    "        r=0,\n",
    "        b=0,\n",
    "        t=0\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "py.iplot(fig, filename='generated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tflearn.fully_connected(layer, layer_sizes[n_layers - 1][0], activation='relu', weights_init='xavier', name=name, weight_decay=weight_decay, reuse=reuse, scope=scope_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gan.save_model(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from  plotly.offline import iplot,init_notebook_mode\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# plotly.tools.set_credentials_file(username='Vahe1994', api_key='MTEFahTX43GTctGnrfox')\n",
    "def get_plot(pclouds=None,id_of_obj = 0,name = 'chair',mode=False):\n",
    "    init_notebook_mode(mode)\n",
    "    x, y, z = pclouds[id_of_obj,:,0],pclouds[id_of_obj,:,1],pclouds[id_of_obj,:,2]\n",
    "    trace1 = go.Scatter3d(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        z=z,\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=4,\n",
    "            line=dict(\n",
    "                color='rgba(217, 217, 217, 0.14)',\n",
    "                width=0.5\n",
    "            ),\n",
    "            opacity=0.8\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    data = [trace1]\n",
    "    layout = go.Layout(\n",
    "        margin=dict(\n",
    "            l=0,\n",
    "            r=0,\n",
    "            b=0,\n",
    "            t=0\n",
    "        )\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    if mode:\n",
    "        return py.iplot(fig, filename= name)\n",
    "    else:\n",
    "        return iplot(fig, filename= name)\n",
    "# def add_points(pclouds,count):\n",
    "#     for pc in pclouds:\n",
    "        \n",
    "def get_conf(experiment_name = 'AE2048',n_pc_points = 2048,bneck_size = 128,epoch=500 ):\n",
    "    top_out_dir = '../data/'                        # Use to write Neural-Net check-points etc.\n",
    "    top_in_dir = '../data/shape_net_core_uniform_samples_2048/' # Top-dir of where point-clouds are stored.\n",
    "    ae_loss = 'emd'                             # Loss to optimize: 'emd' or 'chamfer'\n",
    "    train_dir = create_dir(osp.join(top_out_dir, experiment_name))\n",
    "    train_params = default_train_params()\n",
    "    encoder, decoder, enc_args, dec_args = mlp_architecture_ala_iclr_18(n_pc_points, bneck_size)\n",
    "    conf = Conf(n_input = [n_pc_points, 3],\n",
    "                loss = ae_loss,\n",
    "                training_epochs = epoch,\n",
    "                batch_size = train_params['batch_size'],\n",
    "                denoising = train_params['denoising'],\n",
    "                learning_rate = train_params['learning_rate'],\n",
    "                train_dir = train_dir,\n",
    "                loss_display_step = train_params['loss_display_step'],\n",
    "                saver_step = train_params['saver_step'],\n",
    "                z_rotate = train_params['z_rotate'],\n",
    "                encoder = encoder,\n",
    "                decoder = decoder,\n",
    "                encoder_args = enc_args,\n",
    "                decoder_args = dec_args\n",
    "               )\n",
    "    conf.experiment_name = experiment_name\n",
    "    conf.held_out_step = 5              # How often to evaluate/print out loss on held_out data (if any).\n",
    "    conf.save(osp.join(train_dir, 'configuration'))\n",
    "    return conf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Train_AE(train_ae = False,data = None,conf=None,URL = '/home/service/ThreeDLAPGAN/data/single_class_ae/'):\n",
    "    if train_ae:\n",
    "        reset_tf_graph()\n",
    "        ae = PointNetAutoEncoder(conf.experiment_name, conf)\n",
    "        buf_size = 1 # flush each line\n",
    "        all_pc_data  = PointCloudDataSet(data)\n",
    "        fout = open(osp.join(conf.train_dir, 'train_stats.txt'), 'a', buf_size)\n",
    "        train_stats = ae.train(all_pc_data, conf, log_file=fout)\n",
    "        fout.close()\n",
    "    else:\n",
    "        reset_tf_graph()\n",
    "        ae = PointNetAutoEncoder(conf.experiment_name, conf)\n",
    "        ae.restore_model(model_path=URL,epoch=epoch)\n",
    "    return ae\n",
    "def get_latent(ae,feed_pc):\n",
    "    reconstructions = ae.reconstruct(feed_pc)\n",
    "    latent_codes = ae.transform(feed_pc)\n",
    "    return latent_codes\n",
    "def Train_GAN(experiment_name = 'GAN1',gan=None,generator=None,discriminator=None,data=None):\n",
    "    buf_size = 1 # flush each line\n",
    "    train_stats=[]\n",
    "    top_out_dir = '../data/'\n",
    "    train_dir = create_dir(osp.join(top_out_dir, experiment_name))\n",
    "    fout = open(osp.join(train_dir, 'train_stats.txt'), 'wb', buf_size)\n",
    "    minim_pc_data = PointCloudDataSet(data)\n",
    "    for i in range(1000):\n",
    "        feed_pc, feed_model_names, _ = minim_pc_data.next_batch(1000)\n",
    "        reconstructions = ae.reconstruct(feed_pc)\n",
    "        latent_codes = ae.transform(feed_pc)\n",
    "        lc = PointCloudDataSet(latent_codes)\n",
    "        train_stats=gan._single_epoch_train(lc,  batch_size=500, noise_params=noise_params)\n",
    "        if(i%20==0):\n",
    "            clear_output()\n",
    "            fout.write(train_stats)\n",
    "            print(train_stats)\n",
    "    fout.close()\n",
    "    return gan\n",
    "def Restore_AE(name ='AE512',epoch = 500,conf=None):\n",
    "    top_dir ='../data/'\n",
    "    direct =top_dir+name +'/'\n",
    "    reset_tf_graph()\n",
    "    ae = PointNetAutoEncoder(conf.experiment_name, conf)\n",
    "    ae.restore_model(model_path=direct,epoch=500)\n",
    "    return ae\n",
    "\n",
    "def get_smart_upsampling(data,pc_count,num_neighbors=5):\n",
    "    buf_size=1\n",
    "    fout = open('log.txt', 'wb', buf_size)\n",
    "    pclouds_smart = np.empty([data.shape[0],data.shape[1]+pc_count, data.shape[2]], dtype=np.float32)\n",
    "    for i,pc in tqdm(enumerate(data)):\n",
    "        while True:\n",
    "            try:\n",
    "                fout.write(str(i)+'\\n')\n",
    "                knn  = NearestNeighbors(n_neighbors=num_neighbors)\n",
    "\n",
    "                knn.fit(pc)\n",
    "\n",
    "                neighbors = knn.kneighbors(pc, return_distance=False)\n",
    "                pclouds_smart[i,:pc_count] = pc\n",
    "                pclouds_smart[i,pc_count:] = pc[knn.kneighbors(pc, return_distance=False)].mean(axis=1)\n",
    "                break\n",
    "            except:\n",
    "                continue\n",
    "    fout.close()\n",
    "    return pclouds_smart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclouds_smart2048[:int(2.0/3.0*pclouds_smart2048.shape[0])].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae2048 = Train_AE(train_ae = True,data = pclouds_smart2048[:int(2.0/3.0*pclouds_smart2048.shape[0])],conf=get_conf(experiment_name = 'AE2048_v5_0.6',n_pc_points = 2048,bneck_size = 128,epoch=1000),URL = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae1024 = Train_AE(train_ae = True,data = pclouds_smart1024[:int(2.0/3.0*pclouds_smart1024.shape[0])],conf=get_conf(experiment_name = 'AE1024_v5_0.6',n_pc_points = 1024,bneck_size = 128,epoch=1000),URL = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ae1024 = Train_AE(train_ae = True,data = pclouds_smart1024[:pclouds_smart1024.shape[0]/2],conf=get_conf(experiment_name = 'AE1024',n_pc_points = 1024,bneck_size = 128,epoch=400),URL = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_params = default_train_params()\n",
    "# ae512 = Restore_AE(name ='AE512',epoch = 700,conf = get_conf(experiment_name='AE512',n_pc_points=512,epoch=700))\n",
    "# ae1024 = Restore_AE(name ='AE1024',epoch = 700,conf = get_conf(experiment_name='AE1024',n_pc_points=1024,epoch=700))\n",
    "ae2048 = Restore_AE(name ='AE2048',epoch = 500,conf = get_conf(experiment_name='AE2048',n_pc_points=2048,epoch=700))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pclouds = ae2048.decode(lt)\n",
    "a= pclouds_smart2048[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lt = get_latent(ae2048,a)\n",
    "lt= get_latent(ae2048,pclouds_smart2048[:4])\n",
    "\n",
    "pclouds = ae2048.decode(lt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae1024.decode(lt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclouds[139]=ae512.decode(3.0/4*lt[600]+lt[78]*1.0/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot(pclouds=pclouds,id_of_obj=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get_plot(pclouds=pclouds,id_of_obj=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot(pclouds=pclouds,id_of_obj=139)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = NearestNeighbors(n_neighbors=4)\n",
    "knn.fit(pclouds_smart512[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclouds_smart512[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.kneighbors(pclouds_smart512[0], return_distance=False)[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = pclouds_smart512[0][knn.kneighbors(pclouds_smart512[0], return_distance=False)].mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclouds_smart512[0][knn.kneighbors(pclouds_smart512[0], return_distance=False)[1]].mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = get_smart_upsampling(pclouds,1024,neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls[0][:,0].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(ls[0],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot(pclouds=ls,id_of_obj=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot(pclouds=ls,id_of_obj=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclouds_smart512.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = get_latent(ae=ae512,feed_pc=pclouds_smart512[:5000])\n",
    "\n",
    "k = get_latent(ae=ae512,feed_pc=pclouds_smart512[5000:])\n",
    "\n",
    "lt512 = (np.concatenate((lt,k),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "TASK=2\n",
    "def sample_noise(N,NOISE_DIM = 128):\n",
    "    return np.random.normal(size=(N,NOISE_DIM)).astype(np.float32)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim, out_dim, hidden_dim=128):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(noise_dim, hidden_dim)\n",
    "        nn.init.xavier_normal(self.fc1.weight)\n",
    "        nn.init.constant(self.fc1.bias, 0.0)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, out_dim)\n",
    "        nn.init.xavier_normal(self.fc2.weight)\n",
    "        nn.init.constant(self.fc2.bias, 0.0)\n",
    "        \n",
    "\n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "            Generator takes a vector of noise and produces sample\n",
    "        \"\"\"\n",
    "        h1 = F.relu(self.fc1(z))\n",
    "        y_gen = self.fc2(h1)\n",
    "        return y_gen\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim=100):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(in_dim, 256)\n",
    "        nn.init.xavier_normal(self.fc1.weight)\n",
    "        nn.init.constant(self.fc1.bias, 0.0)\n",
    "        \n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        nn.init.xavier_normal(self.fc2.weight)\n",
    "        nn.init.constant(self.fc2.bias, 0.0)\n",
    "        \n",
    "        self.fc3 = nn.Linear(512, 1)\n",
    "        nn.init.xavier_normal(self.fc3.weight)\n",
    "        nn.init.constant(self.fc3.bias, 0.0)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = F.relu(self.fc1(x))\n",
    "        h2 = F.relu(self.fc2(h1))\n",
    "        if TASK == 1 or TASK ==2:\n",
    "            score = F.sigmoid(self.fc3(h2))\n",
    "        else:\n",
    "            score = self.fc3(h2)\n",
    "        return score\n",
    "class UnitNormClipper(object):\n",
    "\n",
    "    def __init__(self, frequency=5):\n",
    "        self.frequency = frequency\n",
    "\n",
    "    def __call__(self, module):\n",
    "        # filter the variables to get the ones you want\n",
    "        if hasattr(module, 'weight'):\n",
    "            w = module.weight.data\n",
    "\n",
    "            w.clamp_(-0.001,0.001)\n",
    "def g_loss(input_1):\n",
    "    # if TASK == 1: \n",
    "    #     do something\n",
    "    if TASK ==1 :\n",
    "        return torch.mean(torch.log(1-input_1)).cuda()\n",
    "    if TASK == 2:\n",
    "        return -torch.mean(torch.log(input_1)).cuda()\n",
    "    if TASK == 3 or TASK == 4:\n",
    "        return -torch.mean(input_1)\n",
    "    return # TODO\n",
    "def d_loss(input_1,input_2,penalty =None) :\n",
    "    if TASK == 1 or TASK == 2: \n",
    "        return -torch.mean(torch.log(input_2)).cuda() - torch.mean(torch.log(1-input_1)).cuda()\n",
    "\n",
    "    if TASK==3 or TASK == 4:\n",
    "        if(penalty is None):\n",
    "            return -torch.mean(input_2).cuda() + torch.mean(input_1).cuda()\n",
    "        else:\n",
    "            return -torch.mean(input_2).cuda() + torch.mean(input_1).cuda() + penalty.cuda()\n",
    "################################\n",
    "def iterate_minibatches(X, batchsize, y=None):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    \n",
    "    for start in range(0, X.shape[0], batchsize):\n",
    "        end = min(start + batchsize, X.shape[0])\n",
    "        if y is None:\n",
    "            yield X[perm[start:end]]\n",
    "        else:\n",
    "            yield X[perm[start:end]], y[perm[start:end]]\n",
    "def iterate_minibatches_with_inf(X,Inf, batchsize, y=None):\n",
    "    perm = np.random.permutation(X.shape[0])\n",
    "    \n",
    "    for start in range(0, X.shape[0], batchsize):\n",
    "        end = min(start + batchsize, X.shape[0])\n",
    "        if y is None:\n",
    "            yield X[perm[start:end]],Inf[perm[start:end]]\n",
    "        else:\n",
    "            yield X[perm[start:end]], y[perm[start:end]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_512 = Generator(128, out_dim = 128).cuda()\n",
    "discriminator = Discriminator(in_dim = 128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "num_epochs = 200\n",
    "batch_size = 50\n",
    "learning_rate=0.0001\n",
    "\n",
    "# ===========================\n",
    "# IMPORTANT PARAMETER:\n",
    "# Number of D updates per G update\n",
    "# ===========================\n",
    "k_d, k_g = 1, 1\n",
    "if TASK ==3 or TASK==4 or TASK==2 or Task==1:\n",
    "#     g_optimizer = optim.RMSprop(generator.parameters(), lr=1e-4)\n",
    "#     d_optimizer = optim.RMSprop(discriminator.parameters(), lr=1e-4)\n",
    "    lr =1e-4\n",
    "    g_optimizer = optim.Adam(generator_512.parameters(),lr=lr)\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "    k_d = 1\n",
    "    k_g = 1\n",
    "accs = []\n",
    "clipper = UnitNormClipper()\n",
    "use_cuda=True\n",
    "data = lt512\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        ls_g=[]\n",
    "        ls_d=[]\n",
    "        for input_data in iterate_minibatches(data, batch_size):\n",
    "            \n",
    "            # Optimize D\n",
    "            \n",
    "            for _ in range(k_d):\n",
    "                # Sample noise\n",
    "                noise = Variable(torch.Tensor(sample_noise(len(input_data))).cuda())\n",
    "                \n",
    "                # Do an update\n",
    "             \n",
    "                inp_data = Variable(torch.Tensor(input_data).cuda())\n",
    "                data_gen = generator_512(noise)\n",
    "                if(TASK==4):\n",
    "                    #COde from here https://github.com/EmilienDupont/wgan-gp\n",
    "                    alpha = torch.rand(batch_size, 1,)\n",
    "                    alpha = alpha.expand_as(inp_data)\n",
    "                    if use_cuda:\n",
    "                        alpha = alpha.cuda()\n",
    "                    interpolated = alpha * inp_data.data + (1 - alpha) * data_gen.data\n",
    "                    interpolated = Variable(interpolated, requires_grad=True)\n",
    "                    if use_cuda:\n",
    "                        interpolated = interpolated.cuda()\n",
    "\n",
    "                    # Calculate probability of interpolated examples\n",
    "                    prob_interpolated = discriminator(interpolated)\n",
    "\n",
    "                    # Calculate gradients of probabilities with respect to examples\n",
    "                    gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                                           grad_outputs=torch.ones(prob_interpolated.size()).cuda() if use_cuda else torch.ones(\n",
    "                                           prob_interpolated.size()),\n",
    "                                           create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "                    # Gradients have shape (batch_size, num_channels, img_width, img_height),\n",
    "                    # so flatten to easily take norm per example in batch\n",
    "                    gradients = gradients.view(batch_size, -1)\n",
    "                   \n",
    "\n",
    "                    # Derivatives of the gradient close to 0 can cause problems because of\n",
    "                    # the square root, so manually calculate norm and add epsilon\n",
    "                    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "\n",
    "                    # Return gradient penalty\n",
    "                    penalty = 10 * ((gradients_norm - 1) ** 2).mean()\n",
    "                    loss = d_loss(discriminator(data_gen), discriminator(inp_data),penalty)\n",
    "                else:\n",
    "                    loss = d_loss(discriminator(data_gen), discriminator(inp_data))\n",
    "                ls_d.append(loss.data.cpu().numpy()[0])\n",
    "                d_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                d_optimizer.step()\n",
    "                if TASK == 3:\n",
    "                    discriminator.apply(clipper)\n",
    "\n",
    "    \n",
    "            # Optimize G\n",
    "            for _ in range(k_g):\n",
    "                # Sample noise\n",
    "                noise = Variable(torch.Tensor(sample_noise(len(input_data))).cuda())\n",
    "                \n",
    "                # Do an update\n",
    "                data_gen = generator_512 (noise)\n",
    "                loss = g_loss(discriminator(data_gen))\n",
    "                ls_g.append(loss.data.cpu().numpy()[0])\n",
    "                g_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                g_optimizer.step()\n",
    "        if(epoch%10==0):\n",
    "            print('generator_loss:',np.mean(ls_g),'discriminator_loss',np.mean(ls_d))\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot(ae2048.decode(get_latent(ae2048,pclouds_smart2048[:140])),0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot(pclouds_smart2048,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = generator(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen.data.cpu().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclouds = ae512.decode(data_gen.data.cpu().numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator(256, out_dim = 128).cuda()\n",
    "discriminator = Discriminator(in_dim = 128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_ups = get_smart_upsampling(pclouds_smart512[pclouds_smart512.shape[0]/2:],512,num_neighbors=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = get_latent(ae=ae1024,feed_pc=pclouds_smart1024[:1000])\n",
    "for i in np.arange(1000,pclouds_smart1024.shape[0],1000):\n",
    "    lt =(np.concatenate((lt,get_latent(ae=ae1024,feed_pc=pclouds_smart1024[i:i+1000])),axis=0)) \n",
    "lt1024 = lt[pclouds_smart1024.shape[0]/2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_ups[:1000].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = get_latent(ae=ae1024,feed_pc=pc_ups[:1000])\n",
    "for i in np.arange(1000,pc_ups.shape[0],1000):\n",
    "    lt =(np.concatenate((lt,get_latent(ae=ae1024,feed_pc=pc_ups[i:i+1000])),axis=0)) \n",
    "lt_ups1024 = lt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lt1024.shape)\n",
    "print(inform.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 50\n",
    "learning_rate=0.0001\n",
    "\n",
    "# ===========================\n",
    "# IMPORTANT PARAMETER:\n",
    "# Number of D updates per G update\n",
    "# ===========================\n",
    "k_d, k_g = 1, 1\n",
    "if TASK ==3 or TASK==4 or TASK==2 or Task==1:\n",
    "#     g_optimizer = optim.RMSprop(generator.parameters(), lr=1e-4)\n",
    "#     d_optimizer = optim.RMSprop(discriminator.parameters(), lr=1e-4)\n",
    "    lr =1e-4\n",
    "    g_optimizer = optim.Adam(generator.parameters(),lr=lr)\n",
    "    d_optimizer = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "    k_d = 1\n",
    "    k_g = 1\n",
    "accs = []\n",
    "clipper = UnitNormClipper()\n",
    "use_cuda=True\n",
    "data = lt1024\n",
    "inform = lt_ups1024\n",
    "\n",
    "try:\n",
    "    for epoch in range(num_epochs):\n",
    "        ls_g=[]\n",
    "        ls_d=[]\n",
    "        for input_data,info in iterate_minibatches_with_inf(data,inform, batch_size):\n",
    "            \n",
    "            # Optimize D\n",
    "            \n",
    "            for _ in range(k_d):\n",
    "                # Sample noise\n",
    "                noise = Variable(torch.cat((torch.Tensor(sample_noise(len(input_data))),torch.Tensor(input_data-info)),1).cuda())\n",
    "                # Do an update\n",
    "                inp_data = Variable(torch.Tensor(input_data).cuda())\n",
    "                data_gen = generator(noise)\n",
    "                if(TASK==4):\n",
    "                    #COde from here https://github.com/EmilienDupont/wgan-gp\n",
    "                    alpha = torch.rand(batch_size, 1,)\n",
    "                    alpha = alpha.expand_as(inp_data)\n",
    "                    if use_cuda:\n",
    "                        alpha = alpha.cuda()\n",
    "                    interpolated = alpha * inp_data.data + (1 - alpha) * data_gen.data\n",
    "                    interpolated = Variable(interpolated, requires_grad=True)\n",
    "                    if use_cuda:\n",
    "                        interpolated = interpolated.cuda()\n",
    "\n",
    "                    # Calculate probability of interpolated examples\n",
    "                    prob_interpolated = discriminator(interpolated)\n",
    "\n",
    "                    # Calculate gradients of probabilities with respect to examples\n",
    "                    gradients = torch.autograd.grad(outputs=prob_interpolated, inputs=interpolated,\n",
    "                                           grad_outputs=torch.ones(prob_interpolated.size()).cuda() if use_cuda else torch.ones(\n",
    "                                           prob_interpolated.size()),\n",
    "                                           create_graph=True, retain_graph=True)[0]\n",
    "\n",
    "                    # Gradients have shape (batch_size, num_channels, img_width, img_height),\n",
    "                    # so flatten to easily take norm per example in batch\n",
    "                    gradients = gradients.view(batch_size, -1)\n",
    "                   \n",
    "\n",
    "                    # Derivatives of the gradient close to 0 can cause problems because of\n",
    "                    # the square root, so manually calculate norm and add epsilon\n",
    "                    gradients_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1) + 1e-12)\n",
    "\n",
    "                    # Return gradient penalty\n",
    "                    penalty = 10 * ((gradients_norm - 1) ** 2).mean()\n",
    "                    loss = d_loss(discriminator(data_gen), discriminator(inp_data),penalty)\n",
    "                else:\n",
    "                    loss = d_loss(discriminator(data_gen + Variable(torch.Tensor(info).cuda(), requires_grad=False)), discriminator(inp_data))\n",
    "                ls_d.append(loss.data.cpu().numpy()[0])\n",
    "                d_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                d_optimizer.step()\n",
    "                if TASK == 3:\n",
    "                    discriminator.apply(clipper)\n",
    "\n",
    "    \n",
    "            # Optimize G\n",
    "            for _ in range(k_g):\n",
    "                # Sample noise\n",
    "                noise = Variable(torch.cat((torch.Tensor(sample_noise(len(input_data))),torch.Tensor(input_data-info)),1).cuda())\n",
    "                # Do an update\n",
    "                data_gen = generator(noise)\n",
    "                loss = g_loss(discriminator(data_gen + Variable(torch.Tensor(info).cuda(), requires_grad=False)))\n",
    "                ls_g.append(loss.data.cpu().numpy()[0])\n",
    "                g_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                g_optimizer.step()\n",
    "        if(epoch%10==0):\n",
    "            print('generator_loss:',np.mean(ls_g),'discriminator_loss',np.mean(ls_d))\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen = generator(noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen.data.cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pclouds1 = ae1024.decode(data_gen.data.cpu().numpy()+info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot(pclouds1,id_of_obj=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_plot(pclouds,id_of_obj=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 ий уровень "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_2048 = Generator(256, out_dim = 128).cuda()\n",
    "discriminator = Discriminator(in_dim = 128).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_ups = get_smart_upsampling(pclouds_smart1024[pclouds_smart2048.shape[0]/2:],1024,num_neighbors=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = get_latent(ae=ae2048,feed_pc=pclouds_smart2048[:1000])\n",
    "for i in np.arange(1000,pclouds_smart2048.shape[0],1000):\n",
    "    lt =(np.concatenate((lt,get_latent(ae=ae2048,feed_pc=pclouds_smart2048[i:i+1000])),axis=0)) \n",
    "lt1024 = lt[pclouds_smart2048.shape[0]/2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lt = get_latent(ae=ae1024,feed_pc=pc_ups[:1000])\n",
    "for i in np.arange(1000,pc_ups.shape[0],1000):\n",
    "    lt =(np.concatenate((lt,get_latent(ae=ae1024,feed_pc=pc_ups[i:i+1000])),axis=0)) \n",
    "lt_ups2048 = lt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
